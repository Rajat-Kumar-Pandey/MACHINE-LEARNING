{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVU2hpAnFIkB0fu9COiy6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajat-Kumar-Pandey/MACHINE-LEARNING/blob/main/compleete_drowsy_modal_with_best_accurrecy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gTrju2zX4Rb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, Callback\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "from PIL import Image\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Script execution started.\")\n",
        "\n",
        "# Enable mixed precision for faster training on GPUs\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_policy(policy)\n",
        "\n",
        "# Check if GPU (CUDA) is available and log the device being used\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    logger.info(f\"GPU available: {physical_devices}\")\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "else:\n",
        "    logger.warning(\"No GPU found, using CPU instead.\")\n",
        "\n",
        "# Set dataset path\n",
        "dataset_path = '/content/drive/MyDrive/Drowssinness data/Resized_Driver_Drowsiness_Dataset'\n",
        "\n",
        "# Check and remove corrupted images\n",
        "def remove_corrupted_images(directory):\n",
        "    logger.info(\"Checking for corrupted images...\")\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            try:\n",
        "                with Image.open(os.path.join(root, file)) as img:\n",
        "                    img.verify()\n",
        "            except Exception:\n",
        "                os.remove(os.path.join(root, file))\n",
        "                logger.info(f\"Removed corrupted file: {os.path.join(root, file)}\")\n",
        "\n",
        "remove_corrupted_images(dataset_path)\n",
        "\n",
        "# Constants\n",
        "SEED = 123\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "N_CLASSES = 2\n",
        "\n",
        "# Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "])\n",
        "logger.info(\"Data augmentation pipeline initialized.\")\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\"\n",
        ").map(lambda x, y: (data_augmentation(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\"\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Build CNN Model\n",
        "def build_cnn_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(N_CLASSES, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.summary()\n",
        "\n",
        "# Save the model immediately after building it\n",
        "cnn_model.save('cnn_drowsiness_baseline_model.h5')\n",
        "logger.info(\"Baseline model saved as 'cnn_drowsiness_baseline_model.h5'.\")\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Custom callback to save the model when validation accuracy exceeds 98%\n",
        "class SaveHighAccuracyModel(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') > 0.98:\n",
        "            logger.info(f\"Validation accuracy exceeded 98% at epoch {epoch + 1}. Saving model...\")\n",
        "            self.model.save(f'cnn_drowsiness_high_accuracy_model_epoch_{epoch + 1}.h5')\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint('cnn_drowsiness_best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "    TensorBoard(log_dir='./logs', histogram_freq=1),\n",
        "    SaveHighAccuracyModel()\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "logger.info(\"Training the CNN model...\")\n",
        "history = cnn_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model and training history\n",
        "cnn_model.save('cnn_drowsiness_final_model.h5')\n",
        "logger.info(\"Final model saved as 'cnn_drowsiness_final_model.h5'.\")\n",
        "with open('training_history.json', 'w') as f:\n",
        "    json.dump(history.history, f)\n",
        "logger.info(\"Training history saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYI8oDZ1aMNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}