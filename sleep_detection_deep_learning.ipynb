{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1b_xtR4Gp1fLjEnuqtCUS1Q7mKyTC6LX8",
      "authorship_tag": "ABX9TyOclBf3JSGsx18YPmqG2Ncp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajat-Kumar-Pandey/MACHINE-LEARNING/blob/main/sleep_detection_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_qpToMDQb_F",
        "outputId": "73fa9dde-190a-4255-9c24-006b4a4bcc9c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 60ms/step - accuracy: 0.5041 - loss: 0.7849 - val_accuracy: 0.5037 - val_loss: 43.0082 - learning_rate: 0.0010\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 43.0082 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5261 - loss: 0.6895 - val_accuracy: 0.5037 - val_loss: 41.8302 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 41.8302 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.5153 - loss: 0.6934 - val_accuracy: 0.5037 - val_loss: 39.6063 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 39.6063 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.5133 - loss: 0.6889 - val_accuracy: 0.5037 - val_loss: 42.1007 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 42.1007 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.5119 - loss: 0.6891 - val_accuracy: 0.5037 - val_loss: 78.0966 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 78.0966 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m172/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5226 - loss: 0.6902\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5224 - loss: 0.6901 - val_accuracy: 0.5037 - val_loss: 47.4266 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 47.4266 - learning_rate: 2.5000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.5186 - loss: 0.6844 - val_accuracy: 0.5037 - val_loss: 52.6044 - learning_rate: 2.5000e-04\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 52.6045 - learning_rate: 2.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.5185 - loss: 0.6880 - val_accuracy: 0.5037 - val_loss: 59.2553 - learning_rate: 1.2500e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 59.2553 - learning_rate: 1.2500e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m173/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5220 - loss: 0.6835\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.5218 - loss: 0.6835 - val_accuracy: 0.5037 - val_loss: 73.1289 - learning_rate: 1.2500e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 73.1289 - learning_rate: 6.2500e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.5169 - loss: 0.6843 - val_accuracy: 0.5037 - val_loss: 73.9265 - learning_rate: 6.2500e-05\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 73.9265 - learning_rate: 6.2500e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.5137 - loss: 0.6848 - val_accuracy: 0.5037 - val_loss: 73.3132 - learning_rate: 3.1250e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 73.3132 - learning_rate: 3.1250e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m171/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5251 - loss: 0.6819\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.5249 - loss: 0.6820 - val_accuracy: 0.5037 - val_loss: 70.3323 - learning_rate: 3.1250e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 70.3323 - learning_rate: 1.5625e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.5177 - loss: 0.6859 - val_accuracy: 0.5037 - val_loss: 69.3381 - learning_rate: 1.5625e-05\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 69.3381 - learning_rate: 1.5625e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.5175 - loss: 0.6834 - val_accuracy: 0.5037 - val_loss: 67.8863 - learning_rate: 7.8125e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 67.8863 - learning_rate: 7.8125e-06\n",
            "Epoch 29/30\n",
            "\u001b[1m174/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5055 - loss: 0.6869\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.5057 - loss: 0.6869 - val_accuracy: 0.5037 - val_loss: 66.8809 - learning_rate: 7.8125e-06\n",
            "Epoch 30/30\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5037 - val_loss: 66.8809 - learning_rate: 3.9063e-06\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Accuracy: 0.5041\n",
            "Precision: 0.5041\n",
            "Recall: 1.0000\n",
            "F1-score: 0.6703\n",
            "AUC-ROC: 0.5000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, LeakyReLU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "# Paths to datasets\n",
        "drowsy_path = '/content/drive/MyDrive/ff_drowsy'\n",
        "non_drowsy_path = '/content/drive/MyDrive/ff_non_drowsy'\n",
        "\n",
        "# Global Settings\n",
        "img_size = (64, 64)  # Target image size\n",
        "padding = 10         # Padding size in pixels\n",
        "batch_size = 32      # Batch size for data augmentation\n",
        "input_shape = (64, 64, 1)  # Input shape for grayscale images\n",
        "\n",
        "# ------------------- Step 1: Preprocessing -------------------\n",
        "def load_and_preprocess_images_with_padding(directory, label, img_size, padding):\n",
        "    \"\"\"Loads and preprocesses images with padding, resizing, and normalization.\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img_name in os.listdir(directory):\n",
        "        img_path = os.path.join(directory, img_name)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img_padded = cv2.copyMakeBorder(img, padding, padding, padding, padding, cv2.BORDER_CONSTANT, value=0)\n",
        "        img_resized = cv2.resize(img_padded, img_size)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        images.append(img_normalized)\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess datasets\n",
        "drowsy_images, drowsy_labels = load_and_preprocess_images_with_padding(drowsy_path, label=1, img_size=img_size, padding=padding)\n",
        "non_drowsy_images, non_drowsy_labels = load_and_preprocess_images_with_padding(non_drowsy_path, label=0, img_size=img_size, padding=padding)\n",
        "\n",
        "# Combine data\n",
        "X = np.concatenate((drowsy_images, non_drowsy_images), axis=0)\n",
        "y = np.concatenate((drowsy_labels, non_drowsy_labels), axis=0)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X = X.reshape((-1, img_size[0], img_size[1], 1))\n",
        "\n",
        "# ------------------- Step 2: Train-Validation-Test Split -------------------\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# One-hot encoding for categorical labels\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=2)\n",
        "y_val_one_hot = to_categorical(y_val, num_classes=2)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "# ------------------- Step 3: Data Augmentation -------------------\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "val_generator = val_datagen.flow(X_val, y_val_one_hot, batch_size=batch_size)\n",
        "\n",
        "# ------------------- Step 4: Model Definition -------------------\n",
        "def build_model(input_shape, learning_rate=0.001, dropout_rate=0.5, l2_lambda=0.001):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(l2_lambda), input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "        Conv2D(64, (3, 3), kernel_initializer='he_normal'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(2, activation='softmax')  # Output layer\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape)\n",
        "\n",
        "# ------------------- Step 5: Training -------------------\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\n",
        "# ------------------- Step 6: Evaluation -------------------\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_probs[:, 1])\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"drowsiness_model.h5\")\n"
      ],
      "metadata": {
        "id": "XrfXiK6wQ1_T",
        "outputId": "139e797f-5f7e-4c8d-ccff-9995c42ea1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e0ddbe6a8135>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drowsiness_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFWrDQkVdt62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}