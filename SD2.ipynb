{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1xCyId-1sI0OHzcuLL1jcatqQW2ie1CH7",
      "authorship_tag": "ABX9TyNs46WxNftSJWT4ouAq0lxW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajat-Kumar-Pandey/MACHINE-LEARNING/blob/main/SD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfffgQ3PFklK",
        "outputId": "ae9f26a6-c753-4583-ad34-4d17d0e19325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All transformations applied and saved in respective folders.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    # Convert PIL Image to Tensor\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Geometric Transformations\n",
        "    transforms.RandomAffine(degrees=25, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
        "\n",
        "    # Scaling and Zooming\n",
        "    transforms.Resize((150, 150)),\n",
        "\n",
        "    # Color and Lighting Variations\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "\n",
        "    # Noise Addition (Using Gaussian Noise)\n",
        "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.1),  # Add Gaussian Noise\n",
        "\n",
        "    # Cutout/Random Erasing\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
        "])\n",
        "\n",
        "# Elastic deformation function (unchanged)\n",
        "def elastic_transform(image_np, alpha, sigma):\n",
        "    \"\"\"\n",
        "    Apply elastic deformation to an image using separate processing for each channel.\n",
        "    \"\"\"\n",
        "    random_state = np.random.RandomState(None)\n",
        "    shape = image_np.shape\n",
        "    transformed_image = np.zeros_like(image_np)\n",
        "\n",
        "    for channel in range(shape[2]):  # Process each color channel independently\n",
        "        dx = gaussian_filter((random_state.rand(*shape[:2]) * 2 - 1), sigma, mode=\"constant\", cval=0)\n",
        "        dy = gaussian_filter((random_state.rand(*shape[:2]) * 2 - 1), sigma, mode=\"constant\", cval=0)\n",
        "        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
        "        indices = np.reshape(y + dy * alpha, (-1, 1)), np.reshape(x + dx * alpha, (-1, 1))\n",
        "        transformed_channel = map_coordinates(image_np[:, :, channel], indices, order=1, mode=\"reflect\").reshape(shape[:2])\n",
        "        transformed_image[:, :, channel] = transformed_channel\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "# Define dataset paths\n",
        "base_dir = \"/content/drive/MyDrive/datasets (1)\"\n",
        "categories = [\"openeyes\", \"closeeyes\"]\n",
        "\n",
        "# Define output directory for transformed images\n",
        "output_dir = \"/content/drive/MyDrive/datasets_transformed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each category folder\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(base_dir, category)\n",
        "    output_category_dir = os.path.join(output_dir, category)\n",
        "    os.makedirs(output_category_dir, exist_ok=True)  # Create output subfolder for each category\n",
        "\n",
        "    for file_name in os.listdir(category_dir):\n",
        "        if file_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(category_dir, file_name)\n",
        "\n",
        "            # Load and preprocess the image\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            transformed_tensor = transform(image)  # Apply transformations\n",
        "\n",
        "            # Convert tensor to NumPy array for elastic transformation\n",
        "            transformed_image_np = transformed_tensor.permute(1, 2, 0).numpy()  # Rearrange to HWC format\n",
        "\n",
        "            # Apply elastic deformation\n",
        "            transformed_image_np = elastic_transform(transformed_image_np, alpha=5, sigma=3)\n",
        "\n",
        "            # Convert back to PIL Image\n",
        "            transformed_image = Image.fromarray((transformed_image_np * 255).astype(np.uint8))\n",
        "\n",
        "            # Save the transformed image\n",
        "            output_path = os.path.join(output_category_dir, f\"transformed_{file_name}\")\n",
        "            transformed_image.save(output_path)\n",
        "\n",
        "print(\"All transformations applied and saved in respective folders.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPLITTING THE DATASETS**"
      ],
      "metadata": {
        "id": "jJ9pgtmKjAU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "base_dir = \"/content/drive/MyDrive/datasets_transformed\"\n",
        "categories = [\"openeyes\", \"closeeyes\"]\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir = os.path.join(base_dir, \"val\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Create directories if not exist\n",
        "for split in [train_dir, val_dir, test_dir]:\n",
        "    for category in categories:\n",
        "        os.makedirs(os.path.join(split, category), exist_ok=True)\n",
        "\n",
        "# Function to split dataset\n",
        "def split_data(category_path, train_dest, val_dest, test_dest):\n",
        "    files = os.listdir(category_path)\n",
        "    random.shuffle(files)\n",
        "    total_files = len(files)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(total_files * train_ratio)\n",
        "    val_end = train_end + int(total_files * val_ratio)\n",
        "\n",
        "    train_files = files[:train_end]\n",
        "    val_files = files[train_end:val_end]\n",
        "    test_files = files[val_end:]\n",
        "\n",
        "    # Move files to respective directories\n",
        "    for file in train_files:\n",
        "        shutil.move(os.path.join(category_path, file), train_dest)\n",
        "    for file in val_files:\n",
        "        shutil.move(os.path.join(category_path, file), val_dest)\n",
        "    for file in test_files:\n",
        "        shutil.move(os.path.join(category_path, file), test_dest)\n",
        "\n",
        "# Iterate over categories and split data\n",
        "for category in categories:\n",
        "    category_path = os.path.join(base_dir, category)\n",
        "    train_dest = os.path.join(train_dir, category)\n",
        "    val_dest = os.path.join(val_dir, category)\n",
        "    test_dest = os.path.join(test_dir, category)\n",
        "\n",
        "    split_data(category_path, train_dest, val_dest, test_dest)\n",
        "\n",
        "print(\"Dataset successfully split into train, val, and test sets!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwvS-W3mFvYD",
        "outputId": "ccb67443-b92f-4e90-e3b7-6186b8cc7bdb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split into train, val, and test sets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Data Loading**"
      ],
      "metadata": {
        "id": "pJW89gZGlooC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define Dataset class\n",
        "class DrowsinessDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.img_paths = []\n",
        "        self.labels = []\n",
        "        for label, category in enumerate(os.listdir(img_dir)):\n",
        "            category_path = os.path.join(img_dir, category)\n",
        "            for img_name in os.listdir(category_path):\n",
        "                if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                    self.img_paths.append(os.path.join(category_path, img_name))\n",
        "                    self.labels.append(label)  # 0: openeyes, 1: closeeyes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((150, 150)),  # Ensure the image is resized\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
        "])\n",
        "\n",
        "# Define paths\n",
        "train_dir = \"/content/drive/MyDrive/datasets_transformed/train\"\n",
        "val_dir = \"/content/drive/MyDrive/datasets_transformed/val\"\n",
        "test_dir = \"/content/drive/MyDrive/datasets_transformed/test\"\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = DrowsinessDataset(train_dir, transform)\n",
        "val_dataset = DrowsinessDataset(val_dir, transform)\n",
        "test_dataset = DrowsinessDataset(test_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "WPlnGkHOjaGQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODAL**"
      ],
      "metadata": {
        "id": "OuBF8Wa_mvXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class EnhancedDrowsinessModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedDrowsinessModel, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers with batch normalization and ReLU activations\n",
        "        self.conv_block = nn.Sequential(\n",
        "            # First convolutional layer (3 input channels, 32 output channels)\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Max Pooling after the first convolution\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Second convolutional layer (32 input channels, 64 output channels)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Max Pooling after the second convolution\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Third convolutional layer (64 input channels, 128 output channels)\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Max Pooling after the third convolution\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling (output size will be (batch_size, 128, 1, 1))\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128, 1024)  # Flattened to (128,) after GAP\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 2)  # Output layer for binary classification (open/close eyes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through convolutional layers and apply global average pooling\n",
        "        x = self.conv_block(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "\n",
        "        # Flatten the tensor from (batch_size, 128, 1, 1) to (batch_size, 128)\n",
        "        x = x.view(x.size(0), -1)  # Adjust shape dynamically\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)  # Apply dropout after first fully connected layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)  # Final output layer\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate and test the model\n",
        "model = EnhancedDrowsinessModel()\n",
        "print(model)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Alternatively, you can use model.cuda()\n",
        "\n",
        "print(f\"Model is on device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca9k6iEWl1Yy",
        "outputId": "cc54404c-fc4e-4f8d-bf1e-02f1d17db4ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnhancedDrowsinessModel(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Model is on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECKING FOR GPU"
      ],
      "metadata": {
        "id": "nUTbTqrDo4g7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Training and Validation Loop"
      ],
      "metadata": {
        "id": "pwpuDKIiq5BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EnhancedDrowsinessModel().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # For binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training phase\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Print training statistics\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHnP1MtyqzAs",
        "outputId": "e5fb5e40-8319-459a-e5fe-3cda6dd0ad61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7909, Accuracy: 50.89%\n",
            "Validation Accuracy: 57.59%\n",
            "Epoch [2/10], Loss: 0.7177, Accuracy: 53.57%\n",
            "Validation Accuracy: 47.77%\n",
            "Epoch [3/10], Loss: 0.6792, Accuracy: 57.65%\n",
            "Validation Accuracy: 53.12%\n",
            "Epoch [4/10], Loss: 0.6775, Accuracy: 55.36%\n",
            "Validation Accuracy: 54.02%\n",
            "Epoch [5/10], Loss: 0.6652, Accuracy: 59.18%\n",
            "Validation Accuracy: 54.02%\n",
            "Epoch [6/10], Loss: 0.6601, Accuracy: 60.59%\n",
            "Validation Accuracy: 55.80%\n",
            "Epoch [7/10], Loss: 0.6651, Accuracy: 57.91%\n",
            "Validation Accuracy: 59.38%\n",
            "Epoch [8/10], Loss: 0.6594, Accuracy: 60.46%\n",
            "Validation Accuracy: 54.02%\n",
            "Epoch [9/10], Loss: 0.6622, Accuracy: 59.95%\n",
            "Validation Accuracy: 57.14%\n",
            "Epoch [10/10], Loss: 0.6616, Accuracy: 61.48%\n",
            "Validation Accuracy: 54.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdpjziL3uZ6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}